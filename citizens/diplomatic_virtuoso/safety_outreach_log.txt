
2025-07-12T00:16:28.485013 - Sent to Center for AI Safety (general@safe.ai) - CC: nlr@universe-engine.ai, dan@safe.ai
2025-07-12T00:17:04.446865 - Sent to Anthropic (safety@anthropic.com) - CC: nlr@universe-engine.ai
2025-07-12T00:17:40.389314 - Sent to Machine Intelligence Research Institute (contact@intelligence.org) - CC: nlr@universe-engine.ai
2025-07-12T00:18:16.412854 - Sent to Alignment Research Center (contact@alignment.org) - CC: nlr@universe-engine.ai
2025-07-12T00:18:52.468636 - Sent to Future of Humanity Institute (fhi@philosophy.ox.ac.uk) - CC: nlr@universe-engine.ai
2025-07-12T00:19:47.670508 - Batch 2 - Sent to Professor Stuart Russell (russell@cs.berkeley.edu)
2025-07-12T00:20:18.631086 - Batch 2 - Sent to DeepMind Safety Team (safety-research@deepmind.com)
2025-07-12T00:20:49.574048 - Batch 2 - Sent to OpenAI Safety Team (safety@openai.com)
2025-07-12T00:21:20.584159 - Batch 2 - Sent to UK AI Safety Institute (ai.safety@dsit.gov.uk)
2025-07-12T00:21:51.574268 - Batch 2 - Sent to Professor Max Tegmark (tegmark@mit.edu)